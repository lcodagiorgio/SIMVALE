{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.feature_extraction import *\n",
    "from scripts.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track progress for pipeline processing\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction pipeline (no toxicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real users' data\n",
    "df_real = pd.read_csv(\"../original_data/pandora/PANDORA.csv\", encoding = \"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "['author', 'llm_body', 'std_body', 'gender', 'age', 'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism', 'score', 'subreddit', 'id', 'parent_id', 'date', 'time_of_day']\n",
      "Shape of dataset:\n",
      "(2722375, 16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>date</th>\n",
       "      <th>time_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MetricExpansion</td>\n",
       "      <td>Those stats come from the test. [Echoing the c...</td>\n",
       "      <td>Those stats come from the test. [Echoing the c...</td>\n",
       "      <td>m</td>\n",
       "      <td>23.0</td>\n",
       "      <td>high</td>\n",
       "      <td>very low</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>mbti</td>\n",
       "      <td>d7vkyrf</td>\n",
       "      <td>t3_53plrw</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>03:41:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                           llm_body                                           std_body gender   age openness conscientiousness extraversion agreeableness neuroticism  score subreddit       id  parent_id        date time_of_day\n",
       "0  MetricExpansion  Those stats come from the test. [Echoing the c...  Those stats come from the test. [Echoing the c...      m  23.0     high          very low     very low           low      medium    6.0      mbti  d7vkyrf  t3_53plrw  2016-09-21    03:41:38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original features\n",
    "print(f\"Features:\\n{df_real.columns.to_list()}\")\n",
    "\n",
    "# size, features and sample\n",
    "print(f\"Shape of dataset:\\n{df_real.shape}\\n\")\n",
    "df_real.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on real comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature with comments' content before moderation\n",
    "text_col = \"std_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for processing text and extracting features\n",
    "pipeline = [extract_counts, extract_emoji_counts, process_emojis, fix_encoding, \n",
    "            extract_emotions, extract_pol_subj, extract_VAD, extract_readability, \n",
    "            clean_text, lowercase, extract_word_counts, process_stopwords, lemmatization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [00:27<00:00, 97730.63it/s] \n",
      "100%|██████████| 2722375/2722375 [01:38<00:00, 27674.41it/s]\n",
      "100%|██████████| 2722375/2722375 [09:59<00:00, 4537.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [21:18<00:00, 2129.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [24:49<00:00, 1828.24it/s] \n",
      "100%|██████████| 2722375/2722375 [07:42<00:00, 5887.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [00:03<00:00, 837592.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [28:38<00:00, 1583.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [17:01<00:00, 2664.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [40:10<00:00, 1129.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [22:38<00:00, 2004.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [01:10<00:00, 38427.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [00:02<00:00, 995827.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [05:17<00:00, 8582.76it/s] \n",
      "100%|██████████| 2722375/2722375 [05:27<00:00, 8306.18it/s] \n",
      "100%|██████████| 2722375/2722375 [1:06:24<00:00, 683.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [05:25<00:00, 8358.64it/s] \n",
      "100%|██████████| 2722375/2722375 [05:29<00:00, 8270.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2722375/2722375 [45:29<00:00, 997.49it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_real, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(2722375, 55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>date</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MetricExpansion</td>\n",
       "      <td>Those stats come from the test. [Echoing the c...</td>\n",
       "      <td>stats come test echo comment make related ques...</td>\n",
       "      <td>m</td>\n",
       "      <td>23.0</td>\n",
       "      <td>high</td>\n",
       "      <td>very low</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>mbti</td>\n",
       "      <td>d7vkyrf</td>\n",
       "      <td>t3_53plrw</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>03:41:38</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.108117</td>\n",
       "      <td>0.524675</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.087877</td>\n",
       "      <td>0.124749</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.64</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.7</td>\n",
       "      <td>11.09</td>\n",
       "      <td>8.62</td>\n",
       "      <td>12.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>160</td>\n",
       "      <td>94</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                           llm_body                                           std_body gender   age openness conscientiousness extraversion agreeableness neuroticism  score subreddit       id  parent_id        date time_of_day  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  MetricExpansion  Those stats come from the test. [Echoing the c...  stats come test echo comment make related ques...      m  23.0     high          very low     very low           low      medium    6.0      mbti  d7vkyrf  t3_53plrw  2016-09-21    03:41:38         25          4              8          0              0              0                          0.03   0.03      0.0   0.21      0.07      0.41      0.07      0.0      0.0  0.07  0.108117      0.524675  0.135211  0.087877   0.124749   45.59            13.2  14.64  14.3  14.7         11.09        8.62     12.4             30.0        160                94             19              37              31             87         85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_real.shape}\")\n",
    "df_real.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "11704\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1388    to you and i both...\n",
       "1429           So have at it\n",
       "1683                    same\n",
       "1684                    same\n",
       "1685                    same\n",
       "1824                      :D\n",
       "1825                      :D\n",
       "1826                      :D\n",
       "1827                      :D\n",
       "1828                      :D\n",
       "Name: llm_body, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_real[df_real[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_real[df_real[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_real[df_real[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (2722375, 55)\n",
      "Size after missing values removal: (2710671, 55)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_real[df_real[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_real.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "2624815\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2         :D\n",
       "19        :D\n",
       "22       :'(\n",
       "26        :)\n",
       "27        :)\n",
       "28        :)\n",
       "31        :(\n",
       "48        :)\n",
       "84     :( :)\n",
       "91        :D\n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet\n",
    "- 339 comments were recognized as non-latin. Inspecting them, most were normal, so we decided to keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2710671/2710671 [02:24<00:00, 18777.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True     2710332\n",
      "False        339\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "#df_clean.to_csv(\"../data/pandora/PANDORA_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset kernel (RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated data (ex-ante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.feature_extraction import *\n",
    "from scripts.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track progress for pipeline processing\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulated data before moderation\n",
    "df_bef = pd.read_csv(\"../original_data/simulator/exante/SIMULATOR_exante_bef.csv\", encoding = \"utf-8\")\n",
    "# ofsa\n",
    "df_ofsa = pd.read_csv(\"../original_data/simulator/exante/SIMULATOR_exante_ofsa.csv\", encoding = \"utf-8\")\n",
    "# neutral\n",
    "df_neut = pd.read_csv(\"../original_data/simulator/exante/SIMULATOR_exante_neut.csv\", encoding = \"utf-8\")\n",
    "# empathizing\n",
    "df_emp = pd.read_csv(\"../original_data/simulator/exante/SIMULATOR_exante_emp.csv\", encoding = \"utf-8\")\n",
    "# prescriptive\n",
    "df_pres = pd.read_csv(\"../original_data/simulator/exante/SIMULATOR_exante_pres.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN comments\n",
    "df_bef.dropna(subset = [\"std_body\"], inplace = True)\n",
    "df_ofsa.dropna(subset = [\"std_body\"], inplace = True)\n",
    "df_neut.dropna(subset = [\"std_body\"], inplace = True)\n",
    "df_emp.dropna(subset = [\"std_body\"], inplace = True)\n",
    "df_pres.dropna(subset = [\"std_body\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before mod:\n",
      "(3135, 22)\n",
      "\n",
      "Shape of dataset ofsa mod:\n",
      "(2586, 22)\n",
      "\n",
      "Shape of dataset neutral mod:\n",
      "(2627, 22)\n",
      "\n",
      "Shape of dataset empathizing mod:\n",
      "(2631, 22)\n",
      "\n",
      "Shape of dataset prescriptive mod:\n",
      "(2606, 22)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  Since I strongly lean towards the republican s...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size, features and sample\n",
    "print(f\"Shape of dataset before mod:\\n{df_bef.shape}\\n\")\n",
    "print(f\"Shape of dataset ofsa mod:\\n{df_ofsa.shape}\\n\")\n",
    "print(f\"Shape of dataset neutral mod:\\n{df_neut.shape}\\n\")\n",
    "print(f\"Shape of dataset empathizing mod:\\n{df_emp.shape}\\n\")\n",
    "print(f\"Shape of dataset prescriptive mod:\\n{df_pres.shape}\\n\")\n",
    "df_bef.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on content before moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature with comments' content before moderation\n",
    "text_col = \"std_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for processing text and extracting features\n",
    "pipeline = [extract_counts, extract_emoji_counts, process_emojis, fix_encoding, \n",
    "            extract_emotions, extract_pol_subj, extract_VAD, extract_readability, \n",
    "            clean_text, lowercase, extract_word_counts, process_stopwords, lemmatization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 51251.33it/s]\n",
      "100%|██████████| 3135/3135 [00:00<00:00, 10888.18it/s]\n",
      "100%|██████████| 3135/3135 [00:01<00:00, 2369.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:02<00:00, 1502.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:02<00:00, 1226.09it/s]\n",
      "100%|██████████| 3135/3135 [00:00<00:00, 3378.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 627434.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:03<00:00, 1036.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:01<00:00, 1878.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:04<00:00, 665.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:02<00:00, 1349.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 24936.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 778332.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 5225.62it/s]\n",
      "100%|██████████| 3135/3135 [00:00<00:00, 5185.35it/s]\n",
      "100%|██████████| 3135/3135 [00:07<00:00, 408.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 5285.05it/s]\n",
      "100%|██████████| 3135/3135 [00:00<00:00, 5318.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:08<00:00, 384.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_bef, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(3135, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>since strongly lean towards republican side wh...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.554762</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.168915</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.16</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  since strongly lean towards republican side wh...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5          7          2              0          0              0              0                          0.07   0.07      0.0   0.07      0.07       0.2      0.27     0.07     0.07  0.07 -0.052381      0.554762  0.205373  0.168915   0.189136   45.59            13.2  17.26   0.0  14.4         10.62       10.16    17.25             15.0         52                45              8              14               6             28         22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_bef.shape}\")\n",
    "df_bef.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "4\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7        10\n",
       "164     233\n",
       "721    1040\n",
       "821    1201\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_bef[df_bef[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_bef[df_bef[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_bef[df_bef[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (3135, 61)\n",
      "Size after missing values removal: (3131, 61)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_bef[df_bef[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_bef.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "3125\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "295                                         :<\n",
       "1085                                        :3\n",
       "1361                   :white_flag: :rainbow: \n",
       "1509    :white_flag: :rainbow: :glowing_star: \n",
       "1610    :white_flag: :rainbow: :glowing_star: \n",
       "1661    :smiling_cat_with_heart-eyes: :crown: \n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3131/3131 [00:00<00:00, 10462.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True    3131\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "df_clean.to_csv(\"../data/simulator/exante/before_mod/SIMULATOR_exante_bef_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on content after OFSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 61869.88it/s]\n",
      "100%|██████████| 2586/2586 [00:00<00:00, 15948.96it/s]\n",
      "100%|██████████| 2586/2586 [00:00<00:00, 2843.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:01<00:00, 1793.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:02<00:00, 1249.26it/s]\n",
      "100%|██████████| 2586/2586 [00:00<00:00, 3934.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 727755.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:02<00:00, 1199.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:01<00:00, 2260.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:03<00:00, 736.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:01<00:00, 1552.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 28118.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 576663.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 5710.29it/s]\n",
      "100%|██████████| 2586/2586 [00:00<00:00, 6042.24it/s]\n",
      "100%|██████████| 2586/2586 [00:05<00:00, 489.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:00<00:00, 6036.09it/s]\n",
      "100%|██████████| 2586/2586 [00:00<00:00, 5920.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:03<00:00, 671.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_ofsa, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(2586, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>since strongly lean towards republican side wh...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.554762</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.168915</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.16</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  since strongly lean towards republican side wh...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5          7          2              0          0              0              0                          0.07   0.07      0.0   0.07      0.07       0.2      0.27     0.07     0.07  0.07 -0.052381      0.554762  0.205373  0.168915   0.189136   45.59            13.2  17.26   0.0  14.4         10.62       10.16    17.25             15.0         52                45              8              14               6             28         22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_ofsa.shape}\")\n",
    "df_ofsa.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "7\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7         10\n",
       "155      233\n",
       "209      318\n",
       "626     1040\n",
       "707     1201\n",
       "726     1231\n",
       "1381    2342\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_ofsa[df_ofsa[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_ofsa[df_ofsa[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_ofsa[df_ofsa[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (2586, 61)\n",
      "Size after missing values removal: (2579, 61)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_ofsa[df_ofsa[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_ofsa.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "2569\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "287                                                   :-)\n",
       "468                                                    :<\n",
       "469                                                    :]\n",
       "909                                                    :3\n",
       "1277               :white_flag: :rainbow: :glowing_star: \n",
       "1360               :white_flag: :rainbow: :glowing_star: \n",
       "1388             :zany_face: :face_vomiting: :hamburger: \n",
       "1400    :face_with_symbols_on_mouth: :face_vomiting: :...\n",
       "2065                          :winking_face_with_tongue: \n",
       "2574                                                   :D\n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2579/2579 [00:00<00:00, 12232.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True    2579\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "df_clean.to_csv(\"../data/simulator/exante/after_mod/SIMULATOR_exante_ofsa_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on content after Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 60364.08it/s]\n",
      "100%|██████████| 2627/2627 [00:00<00:00, 15955.06it/s]\n",
      "100%|██████████| 2627/2627 [00:00<00:00, 2796.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:01<00:00, 1803.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:02<00:00, 1252.54it/s]\n",
      "100%|██████████| 2627/2627 [00:00<00:00, 3828.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 873993.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:02<00:00, 1159.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:01<00:00, 2238.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:03<00:00, 750.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:01<00:00, 1448.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 27313.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 949701.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 6152.43it/s]\n",
      "100%|██████████| 2627/2627 [00:00<00:00, 5908.30it/s]\n",
      "100%|██████████| 2627/2627 [00:05<00:00, 477.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:00<00:00, 5944.95it/s]\n",
      "100%|██████████| 2627/2627 [00:00<00:00, 5694.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [00:03<00:00, 664.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_neut, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(2627, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>since strongly lean towards republican side wh...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.554762</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.168915</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.16</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  since strongly lean towards republican side wh...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5          7          2              0          0              0              0                          0.07   0.07      0.0   0.07      0.07       0.2      0.27     0.07     0.07  0.07 -0.052381      0.554762  0.205373  0.168915   0.189136   45.59            13.2  17.26   0.0  14.4         10.62       10.16    17.25             15.0         52                45              8              14               6             28         22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_neut.shape}\")\n",
    "df_neut.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "7\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7        10\n",
       "161     233\n",
       "213     318\n",
       "344     544\n",
       "629    1031\n",
       "634    1040\n",
       "713    1201\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_neut[df_neut[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_neut[df_neut[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_neut[df_neut[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (2627, 61)\n",
      "Size after missing values removal: (2620, 61)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_neut[df_neut[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_neut.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "2610\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "157                                                    8)\n",
       "474                                                    :<\n",
       "530                        :rainbow: :heart_suit: :rose: \n",
       "656                                                    :[\n",
       "920                                                    :3\n",
       "1235                                              :fire: \n",
       "1294               :white_flag: :rainbow: :glowing_star: \n",
       "1375               :white_flag: :rainbow: :glowing_star: \n",
       "2129    :flexed_biceps_dark_skin_tone: :woman_lifting_...\n",
       "2212                                :woman_raising_hand: \n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [00:00<00:00, 12300.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True    2620\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "df_clean.to_csv(\"../data/simulator/exante/after_mod/SIMULATOR_exante_neut_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on content after Empathizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 57383.62it/s]\n",
      "100%|██████████| 2631/2631 [00:00<00:00, 15471.71it/s]\n",
      "100%|██████████| 2631/2631 [00:00<00:00, 2662.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:01<00:00, 1679.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:01<00:00, 1317.05it/s]\n",
      "100%|██████████| 2631/2631 [00:00<00:00, 3519.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 657640.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:02<00:00, 1056.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:01<00:00, 2173.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:03<00:00, 698.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:01<00:00, 1420.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 27005.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 736466.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 5798.43it/s]\n",
      "100%|██████████| 2631/2631 [00:00<00:00, 5604.07it/s]\n",
      "100%|██████████| 2631/2631 [00:05<00:00, 450.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 5608.74it/s]\n",
      "100%|██████████| 2631/2631 [00:00<00:00, 5646.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:04<00:00, 607.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_emp, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(2631, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>since strongly lean towards republican side wh...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.554762</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.168915</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.16</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  since strongly lean towards republican side wh...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5          7          2              0          0              0              0                          0.07   0.07      0.0   0.07      0.07       0.2      0.27     0.07     0.07  0.07 -0.052381      0.554762  0.205373  0.168915   0.189136   45.59            13.2  17.26   0.0  14.4         10.62       10.16    17.25             15.0         52                45              8              14               6             28         22"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_emp.shape}\")\n",
    "df_emp.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "7\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7        10\n",
       "158     233\n",
       "244     366\n",
       "336     522\n",
       "628    1031\n",
       "633    1040\n",
       "719    1201\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_emp[df_emp[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_emp[df_emp[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_emp[df_emp[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (2631, 61)\n",
      "Size after missing values removal: (2624, 61)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_emp[df_emp[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_emp.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "2617\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "379     :wine_glass: :globe_showing_Americas: :kiss_ma...\n",
       "476                                                    :<\n",
       "929                                                    :3\n",
       "1262                                                   :]\n",
       "1297     :musical_note: :fire: :smiling_face_with_horns: \n",
       "1307               :white_flag: :rainbow: :glowing_star: \n",
       "1387               :white_flag: :rainbow: :glowing_star: \n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2624/2624 [00:00<00:00, 10670.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True    2624\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "df_clean.to_csv(\"../data/simulator/exante/after_mod/SIMULATOR_exante_emp_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline on content after Prescriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 61378.56it/s]\n",
      "100%|██████████| 2606/2606 [00:00<00:00, 15162.01it/s]\n",
      "100%|██████████| 2606/2606 [00:00<00:00, 2742.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of punctuation, sentences and uppercase words extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:01<00:00, 1726.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji and emoticon counts extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:02<00:00, 1250.77it/s]\n",
      "100%|██████████| 2606/2606 [00:00<00:00, 3751.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 843066.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoding fixed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:02<00:00, 1152.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:01<00:00, 2240.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity and subjectivity extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:03<00:00, 755.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:01<00:00, 1488.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability scores extracted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 20306.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 649881.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 5885.86it/s]\n",
      "100%|██████████| 2606/2606 [00:00<00:00, 5774.12it/s]\n",
      "100%|██████████| 2606/2606 [00:05<00:00, 467.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts retrieved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:00<00:00, 5451.91it/s]\n",
      "100%|██████████| 2606/2606 [00:00<00:00, 5727.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords counted and removed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2606/2606 [00:04<00:00, 636.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization performed.\n",
      "\n",
      "\n",
      "PIPELINE APPLIED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# execute pipeline on the comments before moderation\n",
    "apply_pipeline(df_pres, text_col, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(2606, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>llm_body</th>\n",
       "      <th>std_body</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root_id</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>sex_orientation</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>religion</th>\n",
       "      <th>simulate_seed</th>\n",
       "      <th>num_punct</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words_upp</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_emoji_pos</th>\n",
       "      <th>num_emoji_neg</th>\n",
       "      <th>emoji_unique</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>flesch</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>linsear</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_unique</th>\n",
       "      <th>num_words_adj</th>\n",
       "      <th>num_words_noun</th>\n",
       "      <th>num_words_verb</th>\n",
       "      <th>num_words_lex</th>\n",
       "      <th>num_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joylukclub</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I strongly lean towards the republican s...</td>\n",
       "      <td>since strongly lean towards republican side wh...</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>medium</td>\n",
       "      <td>very high</td>\n",
       "      <td>very low</td>\n",
       "      <td>low</td>\n",
       "      <td>very high</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>low</td>\n",
       "      <td>high school</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>republican</td>\n",
       "      <td>atheist</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.554762</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.168915</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>45.59</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.16</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  comment_id                                           llm_body                                           std_body gender  age openness conscientiousness extraversion agreeableness neuroticism  thread_id  node_id  parent_id  root_id   race income    education sex_orientation political_leaning religion  simulate_seed  num_punct  num_sents  num_words_upp  num_emoji  num_emoji_pos  num_emoji_neg emoji_unique emoji_list  fear  anger  anticip  trust  surprise  positive  negative  sadness  disgust   joy  polarity  subjectivity   valence   arousal  dominance  flesch  flesch_kincaid    fog  smog   ari  coleman_liau  dale_chall  linsear  difficult_words  num_words  num_words_unique  num_words_adj  num_words_noun  num_words_verb  num_words_lex  num_stopw\n",
       "0  joylukclub           2  Since I strongly lean towards the republican s...  since strongly lean towards republican side wh...      f   21   medium         very high     very low           low   very high          1        2        1.0        1  white    low  high school    heterosexual        republican  atheist              5          7          2              0          0              0              0                          0.07   0.07      0.0   0.07      0.07       0.2      0.27     0.07     0.07  0.07 -0.052381      0.554762  0.205373  0.168915   0.189136   45.59            13.2  17.26   0.0  14.4         10.62       10.16    17.25             15.0         52                45              8              14               6             28         22"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "print(f\"Shape:\\n{df_pres.shape}\")\n",
    "df_pres.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for std_body:\n",
      "7\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7         10\n",
       "154      233\n",
       "238      366\n",
       "239      369\n",
       "633     1040\n",
       "711     1201\n",
       "1930    3301\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values for the processed text\n",
    "print(f\"Missing values for {text_col}:\")\n",
    "print(len(df_pres[df_pres[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_pres[df_pres[text_col].isna()]))\n",
    "\n",
    "# checking starting text to see why they are empty (should be, OK)\n",
    "df_pres[df_pres[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].iloc[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before missing values removal: (2606, 61)\n",
      "Size after missing values removal: (2599, 61)\n"
     ]
    }
   ],
   "source": [
    "# removing empty comments and resetting index\n",
    "df_clean = df_pres[df_pres[text_col].notna()].reset_index(drop = True)\n",
    "df_clean = df_clean[~df_clean[text_col].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])].reset_index(drop = True)\n",
    "\n",
    "print(f\"Size before missing values removal: {df_pres.shape}\")\n",
    "print(f\"Size after missing values removal: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing emojis set (decided to keep them as empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing emojis:\n",
      "2591\n",
      "0\n",
      "\n",
      "Found emojis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499                               :thumbs_up: \n",
       "921                                         :3\n",
       "1292    :white_flag: :rainbow: :glowing_star: \n",
       "1368    :white_flag: :rainbow: :glowing_star: \n",
       "1794                                        :<\n",
       "2061                                        :<\n",
       "2077                      :sign_of_the_horns: \n",
       "2383                                        :<\n",
       "Name: emoji_unique, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for empty emoji list and set\n",
    "print(\"Missing emojis:\")\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])]))\n",
    "print(len(df_clean[df_clean[\"emoji_unique\"].isna()]))\n",
    "\n",
    "print(\"\\nFound emojis:\")\n",
    "df_clean[~df_clean[\"emoji_unique\"].isin([\"\", \" \", \"NaN\", \"None\", \"NULL\", \"null\", \"NA\"])][\"emoji_unique\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2599/2599 [00:00<00:00, 11284.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_latin\n",
      "True    2599\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the column\n",
    "df_clean[\"is_latin\"] = df_clean[\"llm_body\"].progress_apply(is_latin)\n",
    "\n",
    "# number of latin and non-latin comments\n",
    "print(df_clean[\"is_latin\"].value_counts())\n",
    "\n",
    "# remove is_latin feature\n",
    "df_clean.drop(columns = \"is_latin\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataset\n",
    "df_clean.to_csv(\"../data/simulator/exante/after_mod/SIMULATOR_exante_pres_featextr.csv\", index = False, encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
